{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "## Links\n",
    "High level overview : https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "It's assumed that your using the environment as defined by the provided yaml file.\n",
    "    env_packages.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors Intro\n",
    "\n",
    "- A matrix is a rank 2-tensor but a tensor is a much more general construct than a matrix.\n",
    "    - NB A rank-2 tensor is basically a square array, which is just one of many possible tensors\n",
    "    - NB Tensor can be much higher in dimensionality than a matrix.\n",
    "- A tensor is a matrix whose basis can change.\n",
    "- Physicist's define \"A tensor is what transforms like a tensor\"\n",
    "- Mathematicians define a \"tensor\" to be a multilinear function: a function of several vector variables that is \"linear in each variable separately\". A \"tensor field\" is a \"tensor-valued function\".\n",
    "\n",
    "\n",
    "\n",
    "## Pytorch v Numpy\n",
    "\n",
    "Let's begin by looking at tensors vs numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "\n",
    "x_data = torch.tensor(data)\n",
    "x_np = torch.from_numpy(np.array(data))\n",
    "\n",
    "# Pretty simpy right?\n",
    "print(x_data)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4768, 0.4614, 0.1822],\n",
      "        [0.8042, 0.8949, 0.8578]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# much like numpy pytorch provides some easy to use constructors\n",
    "print(torch.rand((2,3,)))\n",
    "print(torch.ones((3,3,)))\n",
    "print(torch.zeros((3,1,)))\n",
    "# a lot more can be found here \n",
    "# https://pytorch.org/docs/stable/torch.html#tensor-creation-ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Much like numpy they also have attributes\n",
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n",
      "tensor tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [3., 0., 1., 9.]])\n",
      "torch.cat tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [3., 0., 1., 9., 3., 0., 1., 9., 3., 0., 1., 9.]])\n",
      "tensor.mul(tensor) \n",
      " tensor([[ 1.,  0.,  1.,  1.],\n",
      "        [ 1.,  0.,  1.,  1.],\n",
      "        [ 1.,  0.,  1.,  1.],\n",
      "        [ 9.,  0.,  1., 81.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[ 1.,  0.,  1.,  1.],\n",
      "        [ 1.,  0.,  1.,  1.],\n",
      "        [ 1.,  0.,  1.,  1.],\n",
      "        [ 9.,  0.,  1., 81.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[ 3.,  3.,  3., 13.],\n",
      "        [ 3.,  3.,  3., 13.],\n",
      "        [ 3.,  3.,  3., 13.],\n",
      "        [13., 13., 13., 91.]]) \n",
      "\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [3., 0., 1., 9.]]) \n",
      "\n",
      "tensor([[ 6.,  5.,  6.,  6.],\n",
      "        [ 6.,  5.,  6.,  6.],\n",
      "        [ 6.,  5.,  6.,  6.],\n",
      "        [ 8.,  5.,  6., 14.]])\n"
     ]
    }
   ],
   "source": [
    "# there is also many operations that can be performed on a tensor\n",
    "# https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "tensor[3,0]=3\n",
    "tensor[3,3]=9\n",
    "print('tensor',tensor)\n",
    "\n",
    "# Joining tensors \n",
    "# You can use torch.cat to concatenate a sequence of tensors along a given dimension. \n",
    "# See also torch.stack, another tensor joining op that is subtly different from torch.cat\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print('torch.cat',t1)\n",
    "\n",
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "\n",
    "# This computes the matrix multiplication between two tensors\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "# print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n",
    "\n",
    "# Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Autograd\n",
    "\n",
    "\n",
    "From https://pytorch.org/docs/stable/autograd.html\n",
    "\n",
    "\n",
    "torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:CS7643]",
   "language": "python",
   "name": "conda-env-CS7643-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
